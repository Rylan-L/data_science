{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Text prediction using LSTM Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks can be used for text prediction and natural language processing. Keras includes a dataset of 11,228 newswires from Reuters, with labeled over 46 topics. Predicting the topic depends not just on the the previous words, but the sequence in which they are presented. Therefore they are an ideal test case for LSTM neural networks. \n",
    "\n",
    "I built three LSTM models using hte reuters dataset. The first model performed quite well with 97.83% accuracy and 100 LSTM neurons. For the second model, I increased hte numbre of neurons in the LSTM to 200. This increased the training time of the network, but the network still converged to the same accuracy level. Due to this, for the third iteration I went back to 100 neurons and then reduced hte batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
      "2113536/2110848 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters_word_index.json\n",
      "557056/550378 [==============================] - 0s 0us/step\n",
      "# of Training Samples: 8982\n",
      "# of Test Samples: 2246\n",
      "# of Classes: 46\n"
     ]
    }
   ],
   "source": [
    "num_of_words=10000\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=num_of_words, test_split=0.2)\n",
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "\n",
    "print('# of Training Samples: {}'.format(len(x_train)))\n",
    "print('# of Test Samples: {}'.format(len(x_test)))\n",
    "\n",
    "\n",
    "num_classes = max(y_train) + 1\n",
    "print('# of Classes: {}'.format(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. ... 0. 0. 0.]\n",
      "10000\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "#convert words to integer token using Kera's built in tokenizer\n",
    "tokenizer = Tokenizer(num_words=num_of_words)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(x_train[0])\n",
    "print(len(x_train[0]))\n",
    "\n",
    "print(y_train[0])\n",
    "print(len(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only consider the first 200 words within the review\n",
    "max_review_length = 400\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 400, 46)           460000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               58800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 46)                4646      \n",
      "=================================================================\n",
      "Total params: 523,446\n",
      "Trainable params: 523,446\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/3\n",
      "8982/8982 [==============================] - 112s 12ms/step - loss: 0.1481 - acc: 0.9628 - val_loss: 0.0723 - val_acc: 0.9783\n",
      "Epoch 2/3\n",
      "8982/8982 [==============================] - 110s 12ms/step - loss: 0.0720 - acc: 0.9783 - val_loss: 0.0720 - val_acc: 0.9783\n",
      "Epoch 3/3\n",
      "8982/8982 [==============================] - 110s 12ms/step - loss: 0.0719 - acc: 0.9783 - val_loss: 0.0720 - val_acc: 0.9783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c347d03358>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct our model\n",
    "embedding_vecor_length = 46\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(num_of_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(keras.layers.LSTM(100))\n",
    "model.add(keras.layers.Dense(46, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.83%\n",
      "Test loss: 0.07202055565316233\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "print('Test loss:', scores[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM 2/3, tweaking parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first tweak of the LSTM, I increased the number of neurons in the LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 400, 46)           460000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               197600    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 46)                9246      \n",
      "=================================================================\n",
      "Total params: 666,846\n",
      "Trainable params: 666,846\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/3\n",
      "8982/8982 [==============================] - 113s 13ms/step - loss: 0.1192 - acc: 0.9723 - val_loss: 0.0724 - val_acc: 0.9783\n",
      "Epoch 2/3\n",
      "8982/8982 [==============================] - 114s 13ms/step - loss: 0.0722 - acc: 0.9783 - val_loss: 0.0722 - val_acc: 0.9783\n",
      "Epoch 3/3\n",
      "8982/8982 [==============================] - 114s 13ms/step - loss: 0.0722 - acc: 0.9783 - val_loss: 0.0724 - val_acc: 0.9783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c3eeba2ef0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct our model\n",
    "embedding_vecor_length = 46\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(num_of_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(keras.layers.LSTM(200))\n",
    "model.add(keras.layers.Dense(46, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.83%\n",
      "Test loss: 0.07244043153704859\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "print('Test loss:', scores[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweaking batch size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 400, 46)           460000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               58800     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 46)                4646      \n",
      "=================================================================\n",
      "Total params: 523,446\n",
      "Trainable params: 523,446\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/2\n",
      "8982/8982 [==============================] - 577s 64ms/step - loss: 0.0863 - acc: 0.9761 - val_loss: 0.0725 - val_acc: 0.9783\n",
      "Epoch 2/2\n",
      "8982/8982 [==============================] - 579s 64ms/step - loss: 0.0724 - acc: 0.9783 - val_loss: 0.0726 - val_acc: 0.9783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c3f78e1f98>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct our model\n",
    "embedding_vecor_length = 46\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(num_of_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(keras.layers.LSTM(100))\n",
    "model.add(keras.layers.Dense(46, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=2, batch_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.83%\n",
      "Test loss: 0.07255657746366168\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "print('Test loss:', scores[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
